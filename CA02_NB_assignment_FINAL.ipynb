{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudeademogullari/CA-2/blob/main/CA02_NB_assignment_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "CA02: Spam eMail Detection using Naive Bayes\n",
        "\n",
        "Goal: Train a Naive Bayes classifier to predict whether an email is Spam (1) or Not Spam (0).\n",
        "\n",
        "Data:\n",
        "- Training folder: \"./train-mails\"\n",
        "- Test folder: \"./test-mails\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4p_DvtT7sOIr"
      },
      "outputs": [],
      "source": [
        "#Import required libraries\n",
        "import os                        #To list files in folders\n",
        "import numpy as np               #To store feature matrices\n",
        "from collections import Counter  #To count word frequencies\n",
        "\n",
        "#Machine Learning (Naive Bayes) & evaluation\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEpnKrabStTD"
      },
      "source": [
        "Step 1: Build the dictionary\n",
        "\n",
        "We read all training emails and count token frequency.\n",
        "\n",
        "Then, we remove:\n",
        "\n",
        "- Tokens that are not alphabetic (numbers or punctuation)\n",
        "- Single-character tokens\n",
        "\n",
        "Lastly, we keep the 3000 most common words, which become our feature list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jjKF0nIMwz8_"
      },
      "outputs": [],
      "source": [
        "#Function (make_Dictionary) build a dictionary of the 3000 most common words from the training email dataset\n",
        "\n",
        "def make_Dictionary(root_dir):\n",
        "    all_words = []\n",
        "\n",
        "    #List all files in the training directory\n",
        "    emails = [os.path.join(root_dir, f) for f in sorted(os.listdir(root_dir))]\n",
        "\n",
        "    #Read each email and collect words\n",
        "    for mail in emails:\n",
        "        with open(mail) as m:\n",
        "            for line in m:\n",
        "                words = line.split()\n",
        "                all_words += words\n",
        "\n",
        "    #Count word frequencies\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    #Remove non-alphabetic & single-letter words\n",
        "    list_to_remove = list(dictionary)\n",
        "    for item in list_to_remove:\n",
        "        if not item.isalpha() or len(item) == 1:\n",
        "            del dictionary[item]\n",
        "\n",
        "    #Keep the 3000 most common words\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BkitgbgStTD"
      },
      "source": [
        "Step 2: Extract features & labels\n",
        "\n",
        "Each email becomes a vector of length 3000 (a row in a matrix).\n",
        "\n",
        "- Column j represents the j-th dictionary word.\n",
        "- The value stored in that column is how many times that dictionary word appears in the email.\n",
        "  \n",
        "Label rule:\n",
        "- If the filename starts with \"spmsg\", the label = 1 (Spam)\n",
        "- Otherwise, the label = 0 (Not Spam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dmVW5xNlyOFc"
      },
      "outputs": [],
      "source": [
        "#Function (extract_features) converts each email into a numeric feature vector & assigns spam/ham labels\n",
        "\n",
        "def extract_features(mail_dir):\n",
        "    #List files inside the folder\n",
        "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "\n",
        "    #Matrix: rows = emails & columns = 3000 dictionary words\n",
        "    features_matrix = np.zeros((len(files), 3000))\n",
        "    labels = np.zeros(len(files))\n",
        "    #Going through each email one by one\n",
        "    for docID, fil in enumerate(files):\n",
        "        with open(fil) as fi:\n",
        "            for i, line in enumerate(fi):\n",
        "                if i >= 2:  #Skipping the first 2 lines and start from line 3\n",
        "                    words = line.split()\n",
        "                    for word in words: #going through each word in the email\n",
        "                        wordID = 0\n",
        "                        for i, d in enumerate(dictionary):\n",
        "                            if d[0] == word:\n",
        "                                wordID = i\n",
        "                                features_matrix[docID, wordID] = words.count(word)\n",
        "\n",
        "        filename = os.path.basename(fil) #getting the file name only (without the path)\n",
        "\n",
        "        if filename.startswith(\"spmsg\"):  #If the file name starts with \"spmsg\", it is spam (1), otherwise not spam (0)\n",
        "            labels[docID] = 1\n",
        "        else:\n",
        "            labels[docID] = 0\n",
        "\n",
        "    return features_matrix, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJbRElPStTD"
      },
      "source": [
        "Step 3: Train and evaluate (Naive Bayes)\n",
        "\n",
        "1. Build dictionary from training emails\n",
        "2. Convert training emails into feature matrix + labels  \n",
        "3. Convert test emails into feature matrix + labels  \n",
        "4. Train Naive Bayes model  \n",
        "5. Predict and print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zoq-rE7Mx0pp"
      },
      "outputs": [],
      "source": [
        "#Set relative paths to training and testing folders\n",
        "TRAIN_DIR = \"./train-mails\"\n",
        "TEST_DIR  = \"./test-mails\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "db826c97-9980-449e-9e8f-0fa8ad827e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n"
          ]
        }
      ],
      "source": [
        "#Build dictionary from TRAIN data\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
        "\n",
        "#Extract features & labels for TRAIN and TEST\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lXkUz_rZStTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f1bb52-c8b5-4fb7-c5a9-8dc5d5b46c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model using Gaussian Naive Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ]
        }
      ],
      "source": [
        "#Train the Naive Bayes model,  predict labels on TEST data & evaluate performance using accuracy\n",
        "\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "#Train the model\n",
        "model.fit(features_matrix, labels)\n",
        "\n",
        "print(\"Training completed\")\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "#Predict test labels\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "#Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2LluSKMStTE"
      },
      "source": [
        "Weaknesses of current design + possible improvements\n",
        "\n",
        "Weaknesses:\n",
        "- The feature extraction step uses only 1 line of each email (the 3rd line), so most of the email content is left out\n",
        "- Text preprocessing is very minimal (no stopword removal, stemming or lemmatization)\n",
        "- The model uses raw word counts and that can give too much weight to very common words\n",
        "  \n",
        "Improvements:\n",
        "- Use all lines after the email header when extracting features\n",
        "- Add more text preprocessing (ex. removing stopwords & normalizing words using stemming or lemmatization)\n",
        "- Replace raw word counts with TF-IDF features to capture word importance"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}