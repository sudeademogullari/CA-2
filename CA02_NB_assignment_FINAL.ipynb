{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudeademogullari/CA-2/blob/main/CA02_NB_assignment_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "CA02: Spam eMail Detection using Naive Bayes\n",
        "\n",
        "Goal: Train a Naive Bayes classifier to predict whether an email is Spam (1) or Not Spam (0).\n",
        "\n",
        "Data:\n",
        "- Training folder: \"./train-mails\"\n",
        "- Test folder: \"./test-mails\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p_DvtT7sOIr"
      },
      "outputs": [],
      "source": [
        "#Import required libraries\n",
        "import os                        #To list files in folders\n",
        "import numpy as np               #To store feature matrices\n",
        "from collections import Counter  #To count word frequencies\n",
        "\n",
        "#Machine Learning (Naive Bayes) & evaluation\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEpnKrabStTD"
      },
      "source": [
        "Step 1: Build the dictionary\n",
        "\n",
        "We read all training emails and count token frequency.\n",
        "\n",
        "Then, we remove:\n",
        "\n",
        "- Tokens that are not alphabetic (numbers or punctuation)\n",
        "- Single-character tokens\n",
        "\n",
        "Lastly, we keep the 3000 most common words, which become our feature list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjKF0nIMwz8_"
      },
      "outputs": [],
      "source": [
        "#Function (make_Dictionary) build a dictionary of the 3000 most common words from the training email dataset\n",
        "\n",
        "def make_Dictionary(root_dir):\n",
        "    all_words = []\n",
        "\n",
        "    #List all files in the training directory\n",
        "    emails = [os.path.join(root_dir, f) for f in sorted(os.listdir(root_dir))]\n",
        "\n",
        "    #Read each email and collect words\n",
        "    for mail in emails:\n",
        "        with open(mail) as m:\n",
        "            for line in m:\n",
        "                words = line.split()\n",
        "                all_words += words\n",
        "\n",
        "    #Count word frequencies\n",
        "    dictionary = Counter(all_words)\n",
        "\n",
        "    #Remove non-alphabetic & single-letter words\n",
        "    list_to_remove = list(dictionary)\n",
        "    for item in list_to_remove:\n",
        "        if not item.isalpha() or len(item) == 1:\n",
        "            del dictionary[item]\n",
        "\n",
        "    #Keep the 3000 most common words\n",
        "    dictionary = dictionary.most_common(3000)\n",
        "\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BkitgbgStTD"
      },
      "source": [
        "Step 2: Extract features & labels\n",
        "\n",
        "Each email becomes a vector of length 3000 (a row in a matrix).\n",
        "\n",
        "- Column j represents the j-th dictionary word.\n",
        "- The value stored in that column is how many times that dictionary word appears in the email.\n",
        "  \n",
        "Label rule:\n",
        "- If the filename starts with \"spmsg\", the label = 1 (Spam)\n",
        "- Otherwise, the label = 0 (Not Spam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmVW5xNlyOFc"
      },
      "outputs": [],
      "source": [
        "#Function (extract_features) converts each email into a numeric feature vector & assigns spam/ham labels\n",
        "\n",
        "def extract_features(mail_dir):\n",
        "    #List files inside the folder\n",
        "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
        "\n",
        "    #Matrix: rows = emails & columns = 3000 dictionary words\n",
        "    features_matrix = np.zeros((len(files), 3000))\n",
        "    labels = np.zeros(len(files))\n",
        "    #Going through each email one by one\n",
        "    for docID, fil in enumerate(files):\n",
        "        with open(fil) as fi:\n",
        "            for i, line in enumerate(fi):\n",
        "                if i >= 2:  #Skipping the first 2 lines and start from line 3\n",
        "                    words = line.split()\n",
        "                    for word in words: #going through each word in the email\n",
        "                        wordID = 0\n",
        "                        for i, d in enumerate(dictionary):\n",
        "                            if d[0] == word:\n",
        "                                wordID = i\n",
        "                                features_matrix[docID, wordID] = words.count(word)\n",
        "\n",
        "        filename = os.path.basename(fil) #getting the file name only (without the path)\n",
        "\n",
        "        if filename.startswith(\"spmsg\"):  #If the file name starts with \"spmsg\", it is spam (1), otherwise not spam (0)\n",
        "            labels[docID] = 1\n",
        "        else:\n",
        "            labels[docID] = 0\n",
        "\n",
        "    return features_matrix, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJbRElPStTD"
      },
      "source": [
        "Step 3: Train and evaluate (Naive Bayes)\n",
        "\n",
        "1. Build dictionary from training emails\n",
        "2. Convert training emails into feature matrix + labels  \n",
        "3. Convert test emails into feature matrix + labels  \n",
        "4. Train Naive Bayes model  \n",
        "5. Predict and print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoq-rE7Mx0pp"
      },
      "outputs": [],
      "source": [
        "#Set relative paths to training and testing folders\n",
        "TRAIN_DIR = \"./train-mails\"\n",
        "TEST_DIR  = \"./test-mails\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "134lmhauyQxE",
        "outputId": "eae95258-95df-4ece-d544-15a022292801"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './train-mails'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-570040408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Build dictionary from TRAIN data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_Dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reading and processing emails from TRAIN and TEST folders\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Extract features & labels for TRAIN and TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3279874954.py\u001b[0m in \u001b[0;36mmake_Dictionary\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#List all files in the training directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0memails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#Read each email and collect words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train-mails'"
          ]
        }
      ],
      "source": [
        "#Build dictionary from TRAIN data\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "print(\"reading and processing emails from TRAIN and TEST folders\")\n",
        "\n",
        "#Extract features & labels for TRAIN and TEST\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXkUz_rZStTE"
      },
      "outputs": [],
      "source": [
        "#Train the Naive Bayes model,  predict labels on TEST data & evaluate performance using accuracy\n",
        "\n",
        "print(\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "#Train the model\n",
        "model.fit(features_matrix, labels)\n",
        "\n",
        "print(\"Training completed\")\n",
        "print(\"testing trained model to predict Test Data labels\")\n",
        "\n",
        "#Predict test labels\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "\n",
        "print(\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "\n",
        "#Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2LluSKMStTE"
      },
      "source": [
        "Weaknesses of current design + possible improvements\n",
        "\n",
        "Weaknesses:\n",
        "- The feature extraction step uses only 1 line of each email (the 3rd line), so most of the email content is left out\n",
        "- Text preprocessing is very minimal (no stopword removal, stemming or lemmatization)\n",
        "- The model uses raw word counts and that can give too much weight to very common words\n",
        "  \n",
        "Improvements:\n",
        "- Use all lines after the email header when extracting features\n",
        "- Add more text preprocessing (ex. removing stopwords & normalizing words using stemming or lemmatization)\n",
        "- Replace raw word counts with TF-IDF features to capture word importance"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}